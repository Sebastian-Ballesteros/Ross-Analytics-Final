---
title: "FINAL PROJECT"
author: "Sebasti√°n Ballesteros"
date: "2023-03-20"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: lumen
    highlight: z
---
# Project Guidelines
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# loading libraries
```{r}
library(class)
library(caret)
library(ggplot2)
library(gmodels)
library(neuralnet)
library(glmnet)
library(stringr)
library(kernlab)
library(C50)
```

# Data Cleaning
```{r}
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL
```

## Getting Data Ready for Analysis
```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))
```

## Getting Train and Test Samples
```{r}
set.seed(12345)
test_set <- sample(1:nrow(tele_norm), nrow(tele_norm)/2) 

tele_train <- tele_norm[-test_set,]
tele_test  <- tele_norm[test_set,]
```

# Logit Model

## Backwards Regression
```{r eval=FALSE}
logit_model<- step(glm(yyes ~ . ,data=tele_train, family = "binomial"), 
                   direction = c("backward"))

saveRDS(logit_model,"logit_model")
```

### Summary Logit

```{r}
logit_model <- readRDS("logit_model")

summary(logit_model)
```
### Logit Model Results

An activation threshold of 0.21 will be used from now on as it has proven to maximize profits and maximize kappa from other homeworks and assignmets done on this dataset
```{r}
logit_pred<-predict(logit_model, newdata = tele_test , type = "response")

pred<-as.factor(ifelse(logit_pred>=0.21,1,0))
test<-as.factor(tele_test$yyes)

confusionMatrix(pred,test, positive = "1")
```


# Support Vector Machines

## Training an SVM with every kernel
```{r eval = FALSE}
kernels <- c("rbfdot", "polydot", "tanhdot", "vanilladot", "laplacedot", "besseldot", "anovadot", "splinedot")
svms <- c()

for (i in kernels){
  svm_model <- ksvm(as.factor(yyes) ~ ., data = tele_train, kernel = i)
  svms <- append(svms,svm_model)
}

names(svms) <- kernels

saveRDS(svms,"svms")
```
## Predicting with SVM's

```{r eval=FALSE}
SVMs <- readRDS("svms")

SVM_df <- data.frame(rbfdot = predict(SVMs$rbfdot,tele_test))

SVM_df$polydot    <- predict(SVMs$polydot,tele_test)
SVM_df$tanhdot    <- predict(SVMs$tanhdot,tele_test)
SVM_df$vanilladot <- predict(SVMs$vanilladot,tele_test)
SVM_df$laplacedot <- predict(SVMs$laplacedot,tele_test)
SVM_df$besseldot  <- predict(SVMs$besseldot,tele_test)
SVM_df$anovadot   <- predict(SVMs$anovadot,tele_test)
SVM_df$splinedot  <- predict(SVMs$splinedot,tele_test)

saveRDS(SVM_df, "svm_pred")
```

## SVM's performance

```{r}
svms     <- readRDS("svms")
svm_pred <- readRDS("svm_pred")
```

### rbfdot
```{r}
confusionMatrix(as.factor(svm_pred$rbfdot), as.factor(tele_test$yyes))
```

### polydot
```{r}
confusionMatrix(as.factor(svm_pred$polydot), as.factor(tele_test$yyes))
```

### tanhdot
```{r}
confusionMatrix(as.factor(svm_pred$tanhdot), as.factor(tele_test$yyes))
```

### vanilladot
```{r}
confusionMatrix(as.factor(svm_pred$vanilladot), as.factor(tele_test$yyes))
```

### laplacedot
```{r}
confusionMatrix(as.factor(svm_pred$laplacedot), as.factor(tele_test$yyes))
```

### besseldot
```{r}
confusionMatrix(as.factor(svm_pred$besseldot), as.factor(tele_test$yyes))
```

### anovadot
```{r}
confusionMatrix(as.factor(svm_pred$anovadot), as.factor(tele_test$yyes))
```

### splinedot
```{r}
confusionMatrix(as.factor(svm_pred$splinedot), as.factor(tele_test$yyes))
```


# Decision Tree

The first model built is the Decision Tree. The C5.0 function from the C50 library is used to build the model. The formula used in this case is as.factor(yyes) ~ . which means that the model will use all the other columns to predict the "yyes" column, which is the outcome of interest.

## DT
```{r}
DT_model <- C5.0(as.factor(yyes) ~ ., data = tele_train)
```

### Decision Tree results
```{r}
DT_pred <- as.numeric(predict(DT_model, tele_test))-1

confusionMatrix(as.factor(DT_pred),as.factor(tele_test$yyes), positive = "1")
```

# DT Cost Matrix 
```{r}
#Given the original prompt for the project, and the fact that the tune function for tuning does not run on my computer, i have decided to only see if implementing the 6$ revenue if a client purchases and a $1 cost for each call, meaning that if we call and they pick up we get a profit 5, and if we call them and we don't 

cost_matrix <- matrix(c(0,-1,0,5),ncol = 2)

DT_errorcost <- C5.0(as.factor(yyes) ~ ., data = tele_train, costs = cost_matrix)
```
## Cost Matrix DT results
```{r}
errorcost_pred <- predict(DT_errorcost, tele_test)

confusionMatrix(as.factor(errorcost_pred), as.factor(tele_test$yyes))
```

# Final Prediction
```{r}
combined_pred <- ifelse((ifelse(logit_pred>0.21,1,0) * DT_pred * as.numeric(svm_pred$vanilladot)-1) == 1, 1, 0)
confusionMatrix(as.factor(combined_pred),as.factor(tele_test$yyes))
```

# 10 Person List

## Selecting 10 people
```{r}
set.seed(12345)
today_list <- tele_norm[sample(1:nrow(tele_norm),10),]
today_list$yyes
```

## Ensamble prediction
```{r}
pred_today_logit <- ifelse(predict(logit_model, today_list, type = "response")>0.21,1,0)
pred_today_svm   <- predict(svms$vanilladot, today_list)
pred_today_DT    <- predict(DT_model, today_list)
```

### Who to call

call people with position n in the list as indicated below
```{r}
(pred_today_logit * (as.numeric(pred_today_DT)-1) * (as.numeric(pred_today_svm)-1)) == 1
which((pred_today_logit * (as.numeric(pred_today_DT)-1) * (as.numeric(pred_today_svm)-1)) == 1)
```
In this case we dont call any of the 10 randomly sampled people

# Conclusion
 
After running all three level one models, It was evident that the best performing SVM was the Anovadot kernel, but due to the small number of observations, and debugging problems with prediction I opted to run Vanilladot for the 10 person list due to time constraints. Backwards Step-wise regression converged at 23 variables, and the Decision tree was better off without the cost matrix, as kappa did not change from one model to the other. The best model overall, including level one models was the logit with a Kappa of 0.4122.

When it comes to combining models, there are various factors to consider beyond just model accuracy. While the approach taken in this project may not have been optimal, there may be better ways to combine models that can improve prediction results. It's important to evaluate the strengths and weaknesses of each model and determine the best way to leverage their strengths.

Moreover, it's important to consider whether it's even necessary to combine models, it may be more effective to rely on a single model rather than combining multiple models given the guidelines of this project. Depending on the uses of this model one could be favored over another. If you are looking for robustness/never needing to train data, SVM's might be the best. If you are looking to interpret and find relationships in the data, then regression and decision tree might be better. In my opinion either one is more usefull and better at predicting than the combined model given that we only activate if all three models suggest a positive. 

In terms of activation point, there are various approaches that can be used to optimize performance. While reducing false positives is important, which is what the combined model is very good at, it's equally crucial to avoid missing out on true positives. One approach I have used in the past is to use a cost-based approach that takes into account the costs of false positives and false negatives. However, this cost function will depend to the specific call center and the costs associated with different types of errors. as well as the revenue generated by successful calls

It's also important to keep in mind that model accuracy is not always the most important factor to consider. Depending on the specific use case, there may be other factors to prioritize such as profits, and variability of cashflows.

I would consider that this model would be useful to a call center manager because there is an improvement in prediction than picking at random. The increased revenues are not only significant but given the cost function in our last project, make the difference between profitability and bankruptcy. That isnt to say that this model leaves a lot to be desired out of the data, and in terms of maximizing prediction accuracy.
